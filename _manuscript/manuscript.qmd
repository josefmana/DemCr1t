---
lang: en
title: "Evaluation of Diagnostic Concordance Between Algorithms for Parkinson’s Disease Dementia"
shorttitle: "Parkinson's Disease Dementia Level I Criteria"
author:
  - name: Martina Mana
    corresponding: false
    orcid: "0009-0007-4665-3946"
    role:
      - Conceptualization
      - Data curation
      - Writing - original draft
    affiliations:
      - id: "1FM"
        name: "First Faculty of Medicine and General University Hospital in Prague, Charles University, Czech Republic"
        department: "Department of Neurology and Centre of Clinical Neuroscience"
  - name: Josef Mana
    corresponding: false
    orcid: "0000-0002-7817-3978" 
    email: "josef.mana@lf1.cuni.cz"
    role:
      - Conceptualization
      - Data curation
      - Investigation
      - Formal analysis
      - Software
      - Methodology
      - Project administration
      - Validation
      - Writing - original draft
    affiliations:
      - id: "1FM"
        name: "First Faculty of Medicine and General University Hospital in Prague, Charles University, Czech Republic"
        department: "Department of Neurology and Centre of Clinical Neuroscience"
  - name: Tereza Uhrova
    corresponding: false
    email: "tereza.uhrova@vfn.cz"
    role:
      - Investigation
    affiliations:
      - id: "1FM"
        name: "First Faculty of Medicine and General University Hospital in Prague, Charles University, Czech Republic"
        department: "Department of Neurology and Centre of Clinical Neuroscience"
  - name: Robert Jech
    corresponding: false
    orcid: "0000-0002-9732-8947"
    email: "jech@cesnet.cz"
    role:
      - Funding acquisition
      - Resources
      - Writing - review & editing
    affiliations:
      - id: "1FM"
        name: "First Faculty of Medicine and General University Hospital in Prague, Charles University, Czech Republic"
        department: "Department of Neurology and Centre of Clinical Neuroscience"
  - name: Ondrej Bezdicek
    corresponding: true
    orcid: "0000-0002-5108-0181"
    email: "ondrej.bezdicek@gmail.com"
    role:
      - Investigation
      - Data curation
      - Funding acquisition
      - Conceptualization
      - Project administration
      - Supervision
      - Writing - original draft
    affiliations:
      - id: "1FM"
        name: "First Faculty of Medicine and General University Hospital in Prague, Charles University, Czech Republic"
        department: "Department of Neurology and Centre of Clinical Neuroscience"
abstract: ""
keywords:
  - Parkinson’s disease
  - Parkinson's disease dementia
  - level I criteria
format:
 apaquarto-pdf:
    documentmode: man
    keep-tex: true
 apaquarto-docx: 
    toc: false
author-note:
  disclosures:
    financial-support: "Supported by The project National Institute for Neurological Research (Programme EXCELES, ID Project No. LX22NPO5107) - Funded by the European Union – Next Generation EU; Charles University: Cooperatio Program in Neuroscience; General University Hospital in Prague project MH CZ-DRO-VFN64165."
bibliography: bibliography.bib
floatsintext: false
numbered-lines: false
suppress-title-page: false
warning: false
echo: false
---

```{r}
#| label: "import"

library(targets)
library(tidyverse)
library(gt)

upstore <- here::here(tar_config_get("store"))
tar_source(here::here("R"))

data <- tar_read(raw_data, store = upstore)
vars <- tar_read(variables, store = upstore)
crit <- tar_read(pdd_data, store = upstore)$algorithms
rates <- tar_read(rate_summaries, store = upstore)

pres <- tar_read(pdd_data, store = upstore)$PDD |>
  mutate(present = if_else(is.na(PDD), 0, 1)) |>
  select(id, type, present) |>
  pivot_wider(names_from = type, values_from = present) |>
  column_to_rownames("id")
```

# Introduction

Parkinson’s disease (PD) is a neurodegenerative disorder typically characterized by a progressive onset of motor symptoms, including rigidity, bradykinesia, postural instability and resting tremor. Moreover, patients suffer from a range of non-motor impairments [@postuma2015], particularly cognitive decline. This factor might result in Parkinson’s disease dementia (PDD) in a subset of patients [@meireles2012].

According to a recent meta-analysis, approximately one-quarter of PD patients is likely to be diagnosed with PDD [@sousa2022]. However, reported PDD rate estimates vary widely, ranging from 14% up to 55%, depending on the methodological criteria employed [@sousa2022]. Moreover, factors such as patients' sex [@cereda2016], age and disease duration appear to modulate the risk of cognitive decline and PDD [@oh2016; @rana2012].

Despite the clinical relevance of PDD, its diagnosis remains complex. A milestone in research of PDD was the publication of diagnostic criteria established in 2007 by the International Parkinson and Movement Disorder Society (MDS) [@dubois2007]. In these criteria, the MDS introduced a two-levelled system for PDD detection. Level I consists of brief cognitive assessments, while Level II involves comprehensive neuropsychological testing across cognitive domains [@emre2007].

The original Level I algorithm included eight conditions that had to be satisfied simultaneously in order to diagnose probable PDD. These included: 1) diagnosis of PD proposed by the Queen Square Brain Bank; 2) PD onset prior to the PDD emergence; 3) evidence of global cognitive impairment (MMSE score \< 26 points); 4) cognitive deficit interference with the IADL (assessed by the pill questionnaire or caregiver interview); 5) impairment in at least two cognitive domains, namely memory, attention, visuo-constructive abilities and executive function; 6) there was absence of Major Depressive Disorder; 7) absence of delirium; and 8) exclusion of other abnormalities and potential causes of dementia [@dubois2007].

Currently, efforts are focused on refining this PDD diagnostic framework. A recent call for a change pinpoints limitations regarding the original criteria and suggest various updates to enhance their utility [@kulisevsky2024]. Proposed suggestions include replacement of Mini Mental State Examination (MMSE) by Montreal Cognitive Assessment (MoCA), which is more sensitive to PD specific cognitive impairment; expansion of instrumental activities of daily living (IADL) evaluation; inclusion of language assessment; recognition of anxiety as one **of** the neuropsychiatric symptoms relevant in PDD; and intergration of biomarkers.

In the light of these proposals, the current study aims to evaluate the diagnostic concordance between the original MDS Level I PDD criteria [@emre2007; @dubois2007] and a modified framework based on the recent call for change [@kulisevsky2024]. Furthermore, both Level I diagnostic approaches are compared to PDD diagnosed on Level II. The study aims to address the following research objectives (RO): (RO1) To estimate the PDD rate and evaluate the diagnostic variability and concordance across different PDD criteria. (RO2) To identify specific diagnostic components contributing to PDD classification variability across the applied criteria.

# Methods

## Participants

```{r}
#| label: "dates"

continue <- all(!is.na(data$assdate))
stopifnot("Some assessment dates are missing" = continue)

dates <- sapply(c("min", "max"), function(f) {
  date <- do.call(f, list(data$assdate))
  paste(month(date, label = TRUE, abbr = FALSE), year(date))
})
```

This study retrospectively analyzed clinical data from a cohort of patients with PD at the General University Hospital in Prague. All patients were diagnosed with idiopathic PD by a movement disorder specialist according to the MDS Clinical Diagnostic Criteria for PD [@postuma2015]. Clinical records spanning `r dates["min"]` to `r dates["max"]` were examined. All participants were candidates for Deep Brain Stimulation (DBS) treatment and underwent neuropsychological evaluation conducted by a trained clinical psychologist (OB) as part of standard preoperative assessments for DBS eligibility at the General University Hospital in Prague.

## Neuropsychological Assessment

Cognitive performance was evaluated at both Level I and Level II according to the standard MDS battery for Parkinson’s Disease Mild Cognitive Impairment (PD-MCI) [@litvan2012, @bezdicek2017]. Cognitive performance at Level I was assessed by the Mini-Mental State Examination (MMSE) [@stepankova2014; @folstein1975] and the Montreal Cognitive Assessment (MoCA) [@kopecek2016; @nasreddine2005]. The comprehensive neuropsychological assessment at Level II evaluated five cognitive domains through specific tests: attention and working memory assessed by Trail Making Test Part A (TMT-A) [@bezdicek2012; @reitan2004], and WAIS Digit Span Backward (WAIS DSB) [@wechsler1997], executive function by Categorical Verbal Fluency (CF) [@nikolai2015], and subtest from the Prague Stroop Test – Colors (PST-C) [@bezdicek2021], language by the WAIS Similarities subtest [@wechsler1997], and the Boston Naming Test (BNT-60) [@zemanova2016; @kaplan1983], memory by the Rey Auditory Verbal Learning Test (RAVLT) [@frydrychova2018; @bezdicek2013; @rey1964] delayed recall, and the Brief Visual Memory Test–Revised (BVMTR) [@havlik2020; @benedict1997] delayed recall, or WAIS Family Pictures subtest [@wechsler1997] delayed recall, visuospatial function assessed by the Judgment of Line Orientation Test (JoLO) [@benton1983], and Clock Drawing Test (CLOX) [@royall1998].

The Functional Activities Questionnaire (FAQ) [@bezdicek2016; @pfeffer1982] was administered to assess functional impairment. The Beck Depression Inventory-II (BDI-II) [@ciharova2020; @beck1996] and State-Trait Anxiety Inventory (STAI) [@spielberg1983; @mullner1980] were used to assess neuropsychiatric status.

## Diagnostic algorithms for probable Parkinson's Disease Dementia

In this study, we applied three distinct sets of diagnostic algorithms for probable PDD at Level I. The first set was based on the original framework [@dubois2007], which utilized the Mini-Mental State Examination (MMSE) as a global cognitive screening tool, supplemented by assessments of attention, executive function, visuospatial abilities, and memory. The second set of algorithms was based on the recent call for change of dementia diagnostic guidelines [@kulisevsky2024], which advocates for more sensitive cognitive domain assessments in the context of PD. This updated approach incorporated specific items from the Montreal Cognitive Assessment (MoCA). The third approach applied the Czech version of the shortened Montreal Cognitive Assessment (sMoCA) [@bezdicek2020], a time-efficient modification designed to measure global cognitive performance using a reduced testing protocol that omits items providing redundant information. **The sMoCA has been validated in Czech PD cohort [@bezdicek2020] and shown to be sensitive to cognitive deficits while lowering patient burden [@roalf2017a]. We included the sMoCA in our study for its clinical utility in pre-surgical settings, where time restrictions and patients’ fatigue often limit the feasibility of longer assessments. Moreover, the Czech validation study reported comparable diagnostic accuracy between MoCA (AUC = 0.815) and sMoCA (AUC = 0.796) for distinguishing PD-MCI from PD-NC, supporting the sMoCA as a suitable and efficient alternative.**

Lastly, the fourth approach followed the Level II protocol for diagnosis of PDD and Mild Cognitive Impairment in PD (PD-MCI) [@dubois2007; @litvan2012]. The Level II methodology, including the use of a regression-based normative scoring approach, has been detailed in a prior study [@bezdicek2017]. In this study, the thresholds for cognitive impairment at Level II were set at $z \leq -1.5$. All non-cognitive criteria of probable PDD (i.e., diagnosis of PD that developed before dementia and absence of Major Depression, delirium or other abnormalities that obscure diagnosis) held true for all patients in the sample according to the psychiatric and neurological examinations.

```{r}
#| label: "algo-check"

algonum <- sapply(c("mmse", "moca", "smoca", "lvlII"), function(i) {
  subset(crit, group == i) |>
    nrow()
})

continue <- !any(algonum == 0)
stopifnot("Some algorithms are missing" = continue)
```

For each of these diagnostic approaches, we applied two operationalizations of deficits in Instrumental Activities of Daily Living (IADL). First, we utilized FAQ item 9, which approximates the pill questionnaire from the original criteria [@dubois2007] employing a cut-off score of 2 points or higher. Second, we applied the entire Functional Activities Questionnaire (FAQ) as suggested in the call for change [@kulisevsky2024], employing a cut-off score of 7 points or higher based on Czech normative data [@bezdicek2011]. These methodologies resulted in a total of `r sum(algonum)` algorithms, which were distributed across different diagnostic criteria: `r algonum["mmse"]` MMSE-based, `r algonum["moca"]` MoCA-based, `r algonum["smoca"]` sMoCA-based, and `r algonum["lvlII"]` based on the Level II battery (see @fig-tree, @tbl-crits and Appendix @tbl-algos for the exact specification of each algorithm).

**Finally, patients were evaluated for psychiatric symptoms, including depression, delirium, apathy, anxiety and psychosis, by a neuropsychiatrist trained in evaluation of movement disorders patients (TU). Since severe psychiatric symptoms are exclusion criteria for the diagnosis of probable PDD (conditions 6-8), all patients diagnosed with PDD were double-checked for the presence of psychiatric symptoms in the hospital archives.**

```{r}
#| label: "tbl-crits"
#| tbl-cap: "Summary of probable PDD operationalizations compared in the study."

table_algorithms(vars)
```

\[Insert Table 1 here\]

## Statistical Analyses

Following the framework proposed @lundberg2021, in this study we explicitly connect our research objectives and their corresponding theoretical (i.e., targets of inference) and empirical (i.e., data-driven) estimands to statistical estimates. The theoretical estimand refers to a unit-specific quantity defined over a target population and represents the ideal quantity that would address the research question under optimal conditions, such as access to complete population data or perfect experimental control. In contrast, the empirical estimand corresponds to the quantity that is actually computable using the available dataset, given real world constraints. The full description of the study’s estimands and their relation to our research objectives is presented in the Appendix (see @tbl-estimands).

To address study objectives, we started by repeatedly assigning each patient the diagnosis of probable PDD based on each PDD algorithm listed in @tbl-crits (see also @tbl-algos) resulting in a `r sum(data$incl)` (patients) $\times$ `r sum(algonum)` (algorithms) matrix where each cell indicates whether a patient (row) meets criteria for probable PDD according to an algorithm (column). PDD rate estimates were computed as $\frac{N_{PDD}}{N_{total}}$ separately for each algorithm. The predictive value of age and sex was then evaluated by fitting a set of logistic regressions, one for each algorithm for probable PDD, whereby the probable PDD was predicted by age, sex and their interaction.

Next, a set of two class cross-tabulations with associated statistics was computed for each pair of algorithms via the `confusionMatrix()` function from the R package *caret* [@kuhn2008]. For each pair of algorithms, the analysis was repeated twice such that each variable of the pair served once as the reference and once as the predictor. Following measures were used to evaluate pairwise concordance between different algorithms for probable PDD: 1) Cohen's $\kappa$ with its 95% confidence interval (CI) computed via the `cohen.kappa()` function from the R package *psych* [@revelle2024]; 2) Accuracy (i.e., the proportion of correct predictions, both true positives and true negatives, among the total number of cases) with its 95% CI; 3) Sensitivity/Recall (i.e., the proportion of true positives); and 4) Specificity (i.e., the proportion of true negatives).

Finally, the No Information Rate (NIR) was calculated for each pair of algorithms. NIR is the accuracy that could be obtained by always predicting the majority class and in our case it is equivalent to the complement of the PDD rate estimate according to the reference algorithm. Accuracy of prediction was compared to the NIR via a one-sided Exact Binomial Test as implemented by the `binom.test()` R stats function. Reference/predictor pairs associated with p \< .05 were considered to show significantly better accuracy than NIR. In other words, for reference/predictor pairs associated with p \< .05, we conclude that knowing the probable PDD status according to the predictor algorithm helps to estimate the probable PDD status according to the reference algorithm and the two algorithms thus show substantial concordance.

**Missing data were handled by complete cases analysis and pairwise complete cases analysis. In other words, each univariate and analysis pairwise comparison used all available data.** Data wrangling and visualizations were done in the *tidyverse* package [@wickham2019] and tables were formatted in the *gt* package [@iannone2024]. All analyses were conducted within the R (version `r with(version, paste(major, minor, sep="."))`) software environment for statistical computing [@rsoft]. The software code supporting this article is available at <https://github.com/josefmana/demcrit.git>.

# Results

```{r}
#| label: "results"

desc  <- tar_read(sample_description, store = upstore)
algos <- tar_read(algorithms, store = upstore)
pred  <- tar_read(demographic_predictors, store = upstore)
conc  <- tar_read(concordance_statistics, store = upstore)
kappa <- tar_read(kappa_summmaries, store = upstore)
perc  <- rates$table$perc

# Total numbers:
n_total <- sum(data$incl)
id_excl <- names(which(rowSums(pres) == 0))
n_excl <- length(id_excl)
n_incl <- n_total - n_excl
if (n_excl == 1) {
  excl_txt <- paste0("one patient was excluded due to missing neuropsychological data, resulting in a final sample of ", n_incl, " patients")
} else if (n_excl > 1) {
  excl_txt <- paste0(english::english(n_excl), " patients were excluded due to missing neuropsychological data, resulting in a final sample of ", n_incl, " patients")
} else {
  excl_txt <- "no patients was excluded."
}

# Function for writing M (SD):
msd_txt <- function(x, txt = "", tab = desc$table) {
  paste0(
    subset(tab, variable == x)[ , c("M", "SD")] |> paste(collapse = " (SD = "),
    ")",
    txt
  )
}

# In-text demographic data
sex <- subset(desc$table, variable == "Sex")$N
age <- msd_txt("Age", " years of age")
edu <- msd_txt("Education", " years of education")
dur <- msd_txt("PD duration", " years of disease duration")
updrs_off <- msd_txt("UPDRS III off state", " Unified Parkinson Disease Rating Scale (UPDRS), part III in medication OFF state")
updrs_on <- msd_txt("UPDRS III on state", " Unified Parkinson Disease Rating Scale (UPDRS), part III in medication ON state")
bdi <- msd_txt("BDI")
staix1 <- msd_txt("STAI X1")
staix2 <- msd_txt("STAI X2")

# In-text rate estimates
rates_overall <- sapply(c("M", "SD", "Md", "minmax"), \(f) do_summary(perc, 2, f))
rates_FAQ9 <- sapply(c("M", "SD", "Md", "minmax"), \(f) do_summary(perc[grepl("FAQ 9", rates$table$IADL)], 2, f))
rates_FAQtot <- sapply(c("M", "SD", "Md", "minmax"), \(f) do_summary(perc[rates$table$IADL == "FAQ > 7"], 2, f))

# Minimum p-value of logistic regressions:
min_p <- subset(pred$values, quantity == "p-value")$estimate |>
  min() |>
  do_summary(3, "p")
```

## Sample Description

**Total of** `r n_total` patients **were considered for the study, out of which `r excl_txt`**. The sample included `r sex` men, with an average of `r age`, `r edu`, `r dur`, `r updrs_off` and `r updrs_on`. **Descriptive statistics for neuropsychiatric symptoms indicated within average level of depressive and anxiety symptoms in our cohort, with the average BDI-II of `r bdi`, average STAI X1 of `r staix1`, and average STAI X2 of `r staix2`. However, according to the psychiatric assessment, none of the patients with probable PDD was suffering from the major depressive disorder, delirium or other neuropsychiatric abnormalities that would excluded the diagnosis.** Cognitive characteristics of the sample are summarized in @tbl-desc.

```{r}
#| label: "tbl-desc"
#| tbl-cap: "Cognitive characteristics of the sample."

desc$gtable
```

\[Insert Table 2 here\]

## PDD Rate Estimates

```{r}
#| label: "observations-univariate"

obs <- rates$table|>
  count(N) |>
  mutate(perc_missing = glue::glue("{round(100 * (n_incl-N) / n_incl, 1)}%"))
```

Algorithm-wise rate of PDD estimates **corresponding numbers of patients with available data** are presented in @tbl-rates. **In total, there were `r obs[1, "n"]` algorithms based on `r obs[1, "N"]` complete observations (`r obs[1, "perc_missing"]` missing). One algorithm used `r obs[2, "N"]`, `r obs[3, "N"]`, and `r obs[4, "N"]` complete observations, respectively (`r obs[2, "perc_missing"]`, `r obs[3, "perc_missing"]` and `r obs[4, "perc_missing"]` missing). A further `r obs[5, "n"]` algorithms were based on `r obs[5, "N"]` complete observations (`r obs[5, "perc_missing"]` missing), and `r obs[6, "n"]` algorithms included the full sample of `r obs[6, "N"]` patients.**

On average, estimated PDD rate was `r rates_overall["M"]`% (SD = `r rates_overall["SD"]`, Md = `r rates_overall["Md"]`, range `r rates_overall["minmax"]`). Notably, the estimates were substantially lower when FAQ item 9 was used as a criterion of IADL deficit (M = `r rates_FAQ9["M"]`% SD = `r rates_FAQ9["SD"]`, Md = `r rates_FAQ9["Md"]`, range `r rates_FAQ9["minmax"]`) compared to using total FAQ score criterion (M = `r rates_FAQtot["M"]`% SD = `r rates_FAQtot["SD"]`, Md = `r rates_FAQtot["Md"]`, range `r rates_FAQtot["minmax"]`) as demonstrated in @fig-tree **(see also @fig-rates for per-operationalisation distribution of PDD rate estimates)**. Neither age nor sex or their interaction (*p*s $\geq$ `r min_p`) reliably predicted probable PDD classification across algorithms (see @fig-data and @fig-pars).

\[Insert Figure 1 here\]

```{r}
#| label: "fig-tree"
#| fig-cap: "A dendrogram representing algorithms creation."

make_dendrogram(rates$table)
```

## Concordance between Algorithms

```{r}
#| label: "observations-bivariate"

# Dimensions of the results table:
dims <- glue::glue("{dim(conc$table)[1]} rows x {dim(conc$table)[2]} columns")

# Number of observations per algorithm pair:
obs_pairs <- conc$table |>
  count(N) |>
  drop_na() |> # NA for cases where reference == predictor
  mutate(perc = 100 * n / sum(n))

# The most missing values present:
min_miss <- glue::glue("{round(100 * (n_incl - obs_pairs[1, 'N']) / n_incl, 2)}%")

# Percentage of algorithm pairs with N > K-1
K <- 200
perc_above_k <- obs_pairs |>
  filter(N >= K) |>
  pull(perc) |>
  sum() |>
  do_summary(2)
```

Results of the analyses of prediction Accuracy, Cohen's $\kappa$, Sensitivity and Specificity are presented in @fig-acc, @fig-kappa, @fig-sens and @fig-spec respectively. **The number of complete pairwise cases ranged from `r obs_pairs[1, "N"]` (`r min_miss` missing) to `r obs_pairs[nrow(obs_pairs), "N"]` (full sample). Most comparisons (`r perc_above_k`%) were based off of `r K` or more observations.**[^1]

[^1]: **Due to the large number of entries (`r dims` representing pairwise comparisons and metrics of interest respectively), the table with numerical results is not presented here or in the Appendix. Instead, we share the table share as data in the accompanying R package available at <https://github.com/josefmana/demcrit.git>. To obtain the table in format not dependent on R, follow the tutorial at <https://josefmana.github.io/demcrit/articles/concordance.html>**

Generally, algorithms that employed the same operationalization of IADL deficit showed substantial pairwise concordance, however, algorithms that operationalized IADL deficit differently did not. Whereas among algorithms with identical IADL deficit operationalization, the agreement judged by Cohen's $\kappa$ was moderately high (operationalization by FAQ total score: $\kappa$ = `r kappa$sums["faq_tot"]`; operationalization by FAQ item 9: $\kappa$ = `r kappa$sums["faq_9"]`), among algorithms that differ in IADL deficit operationalization but are otherwise identical it was low: $\kappa$ = `r kappa$sums["iadl"]`.

\[Insert Figure 3 here\]

```{r}
#| label: "fig-acc"
#| fig-cap: "Prediction accuracy matrix."
#| apa-note: "The matrix depicts classification accuracy of algorithms for PDD depicted on x-axis in predicting outcomes based on algorithms on the y-axis. Algorithms printed in blue defined IADL deficit by FAQ total score, algorithms printed in black defined IADL deficit by FAQ item 9 response. Cases with asterisk indicate predictive accuracy statistically significantly higher than the No Information Rate."

conc$plots$Accuracy
```

## Prediction of Level II Criteria

```{r}
#| label: "lvlII-stats"

lvlII_rates <- c(
  faq_tot = subset(rates$table, type == "Lvl.II (1)")$perc |> do_summary(2) |> paste0("%"),
  faq_9 = subset(rates$table, type == "Lvl.II (2)")$perc |> do_summary(2) |> paste0("%")
)

smoca_stats <- c(
  spec = subset(conc$table, predictor == "sMoCA (1)" & reference == "Lvl.II (1)")$Specificity |> do_summary(2),
  sens = subset(conc$table, predictor == "sMoCA (1)" & reference == "Lvl.II (1)")$Sensitivity |> do_summary(2)
)
```

For easier interpretability of our results, we next examine cases where Level II algorithms served as a reference and Level I algorithms as a predictor. @tbl-approx shows five Level I algorithms with the highest and five with the lowest accuracy in predicting Level II classification of probable PDD.

When IADL deficit was defined by total FAQ score, the Level II estimate of PDD rate was `r lvlII_rates["faq_tot"]`. All five Level I algorithms that approximated the Level II classification most accurately were MoCA-based and defined Executive Function deficit by Clock drawing rather than Lexical fluency test. On the other hand, two out of the five Level I algorithms with the lowest accuracy were MMSE-based, whereas the remaining three were MoCA-based and defined Executive Function deficit by Lexical fluency test.

When IADL deficit was defined by FAQ item 9 score, the Level II estimate of PDD rate was `r lvlII_rates["faq_9"]`. Overall, the difference between the most accurate and the least accurate Level I algorithms was lower than in the case of IADL deficit being defined by FAQ total score (see @tbl-approx). The five most accurate algorithms were all MoCA-based, defined Executive Function deficit by Clock drawing (with threshold \< 2) and in majority of cases defined Language deficit by Animal naming. Two out of the five Level I algorithms with the lowest accuracy were MMSE-based, whereas the remaining three were MoCA-based and defined Executive Function deficit by Clock drawing (with threshold \< 3) and Language deficit by Abstraction.

Finally, if the predictors are sorted by their balanced accuracy (i.e., average of sensitivity and specificity) instead of raw accuracy, the results are similar with the exception that for prediction of Level II with total FAQ score algorithm for probable PDD, the highest balanced accuracy was achieved by the sMoCA algorithm with sensitivity `r smoca_stats["sens"]` and specificity `r smoca_stats["spec"]` (see @tbl-approx-bal).

```{r}
#| label: "tbl-approx"
#| tbl-cap: "Level I algorithms for probable PDD as predictors of Level II classification as the reference."

table_levelII_approximations(conc$table, algos, "Accuracy_raw")
```

# Discussion

This study systematically investigated the application of multiple Level I diagnostic criteria for PDD. Our results show variability in PDD rate estimates, strongly influenced by the choice of cognitive screening instrument (MMSE, MoCA and sMoCA) and the operationalization of functional impairment. The divergence observed across algorithms demonstrates the sensitivity of diagnostic outcomes to seemingly negligible methodological choices.

## Variability in PDD Rate Estimates

```{r}
#| label: "rates-minmax"

rate_min <- sub("-.*", "", rates_overall["minmax"])
rate_max <- sub(".*-", "", rates_overall["minmax"])
```

Our results showed a wide range in estimated PDD rate across algorithms, ranging from `r rate_min`% to `r rate_max`%. Estimates reached lower rates when using solely FAQ item 9 (as an approximation of the pill questionnaire suggested by @dubois2007) in comparison with the full FAQ scale. This discrepancy highlights the diagnostic importance of how IADLs are assessed.

Our overall PDD rates were consistently lower than previous studies regarding PDD among PD patients, demonstrating wide variability based on various criteria used. For instance, a retrospective study reported a PDD rate of 19.7% [@rana2012], while other clinical investigation found even higher rate, reaching up to 30% [@aarsland2005]. A recent meta-analysis synthesizing global data placed the expected PDD rate in PD at 26.30% [@sousa2022]. Compared to these estimates, our study reports generally lower PDD rates, likely reflecting differences in diagnostic criteria, methodology and sample characteristics. Specifically, our sample was younger compared to other PD cohorts and age was repeatedly shown to be a strong predictor of PDD across studies [@rana2012; @sousa2022].

```{r}
#| label: "age-over-70"

over70 <- data |>
  mutate(older = 100 * (age >= 70)) |>
  pull(older) |>
  mean(na.rm = TRUE) |>
  round(1) |>
  paste0("%")
```

Interestingly, we did not observe any reliable age-related differences in PDD rate within our cohort. This lack of age-dependency may, however, be also stemming from the relatively younger age of our cohort, because previous reports indicate that association between age and PDD is not linear but increases with age and may not reach substantial values before **older age. In both @rana2012 and @oh2016, nine out of ten patients with dementia were 70 years of age or older. In our sample, only `r over70` participants were in this age range. Consequently, studies with older cohorts are probably necessary to detect a robust association between age and the risk of probable PDD.**

## Concordance Between Diagnostic Algorithms

Pairwise comparisons of diagnostic algorithms showed that agreement was notably stronger among those using the same IADL operationalization compared to those using different IADL definitions. Moreover, the agreement was slightly higher between algorithms that defined IADL deficit by FAQ item 9 compared to algorithms that defined it using the full FAQ scale. One possible explanation of this difference follows from the observation that algorithms using the full-scale definition yielded higher PDD rate estimates. Because there was a higher probability of being diagnosed with IADL deficit based on the full FAQ scale, there was also a bigger room for disagreement in the Cognitive Impairment status when different indexes were used (e.g. by defining executive deficit via clock drawing vs. lexical fluency).

Overall, when the same IADL definitions were used across algorithms, we observed concordance levels varying from moderate (using FAQ total score) to strong (using FAQ item 9), consistent with inter-rater reliability analysis [@mchugh2012]. Contrarily, the concordance between algorithms using different IADL deficit definitions was equivalent to minimal agreement. This demonstrates that even slight methodological differences can yield divergent diagnostic outcomes. Such findings are critical for clinicians relying on Level I criteria for eligibility decisions, as the choice of algorithm could lead to contradictory classifications of PDD.

## Predictive Validity Comparison With Level II Criteria

Using Level II diagnosis as the reference, MoCA-based Level I algorithms, particularly those using Clock Drawing to assess executive function, demonstrated the highest predictive accuracy. This supports recent proposals to modernize PDD diagnostic frameworks [@kulisevsky2024], favouring MoCA-derived components and broader functional assessment tools. In contrast, MMSE-based algorithms consistently underperformed, suggesting limited sensitivity in capturing cognitive deficits typical in PDD.

The advantage of MoCA-based algorithm was evident in combination with using the FAQ total score, reaching accuracy rates of 94–99%. Furthermore, in the algorithm using sMoCA, the raw accuracy was moderate, however, the balanced accuracy (i.e. combined sensitivity and specificity) was high. These findings imply that simplified tools such as sMoCA may be a promising tool for heightening efficiency of the assessment while maintaining diagnostic accuracy. **This underscores the clinical utility of sMoCA screening tool. Although raw accuracy was moderate, balanced accuracy results ensure avoidance of bias from class imbalance. From a practical point of view, it suggests that even brief screening tools can achieve reliable classification, which means an advantage for routine clinical settings and time-restricted assessments.**

## Constraints on Generality

**Given the single-center nature of our study and the specific cohort, several not yet discussed constraints on generality apply to our findings. These can be broadly separated into three topics; age distribution, cognitive phenotype and cultural background.**

**In our sample, the younger age and preserved cognitive level might limit age-related effects. This fact could bias algorithm results toward higher specificity but lower sensitivity [@larvin2023]. Previous studies report that DBS candidates tend to be, on average, younger and cognitively less impaired than the general PD population [@bronstein2011], which could bias diagnostic algorithm performance toward overestimating specificity. Therefore, our findings may not be fully generalizable to older or more cognitively impaired people with PD.**

**Previous longitudinal studies of DBS**

**FAQ culturally different.**

**Finally, our results may generalise well to other pre-DBS cohorts.**

## Limitations and Future Directions

This study’s generalizability is limited by its retrospective design and the homogeneity of the patient cohort, which does not reflect broader PD populations with varying cognitive profiles. Specifically, the younger age of the sample and possible lack of high-risk phenotypes for PDD compared to the general population of people with PD might have been responsible for comparatively lower PDD rate estimates in our study.

Nevertheless, the analysis of the algorithm concordance might not be affected by the sample to the same degree allowing for a broader generalization. **However, the use of a DBS cohort offered certain methodological advantages. For example, all patients underwent standardized and comprehensive neuropsychological testing, which resulted in well-characterized and high-quality dataset, allowing us to systematically evaluate multiple diagnostic algorithms.** Future studies should aim to replicate our results on larger and different cohorts **with more heterogenous sample**. To make this process easier, the code used to generate our results is publicly available and easily applicable to similarly structured data.

**Due to the retrospective nature of the study, some patients lacked one or more key measures required for the diagnosis of probable PDD by certain algorithms. Missing data were handled using the pairwise complete cases method. The main advantages of this approach are its straightforward implementation and preservation of statistical power. However, it may introduce bias, particularly when data are not missing completely at random (MCAR) and when causal inference is the goal [@little2019].**

**Modern alternatives, such as multiple imputation techniques, have been shown to produce less biased estimates than case deletion strategies in analyses based on confusion matrices [@karakaya2014], whereas the evidence supporting their superiority in the estimation of Cohen’s $\kappa$ remains limited [@deraadt2019]. These modern approaches also require careful specification of the imputation model and the underlying causal mechanism of missingness to ensure appropriate covariate selection [@long2011; @bianco2023]. For the sake of parsimony, we did not employ advanced missing-data techniques but instead explicitly described the observed missingness patterns. Consequently, our findings should be regarded as exploratory and primarily serve as a basis for hypothesis generation in future, more controlled studies.**

**Another possible avenue for analysing the present data would be to exploit their hierarchical structure by fitting a multilevel model with algorithms nested within cognitive tests (see @fig-tree). Such an approach could leverage partial pooling [@gelman2012] and might approximate modern item response theory frameworks [@burkner2021], where PDD is conceptualised as a latent trait and the individual diagnostic algorithms act as items measuring it. If feasible, this psychometric perspective may represent an exciting new direction for conceptualising complex clinical phenomena such as PDD (cf. @miller2012, @kiselica2021).**

Furthermore, another limitation is the use of the FAQ questionnaire for IADL assessment. FAQ is subjective and informant-reliant, which may result in biased data. **At the same time, the questionnaire may vary across cultures, therefore, the results may not be transferable. For example, activities such as financial management or driving, may not be equally practiced across cultures or contexts, which might complicate interpretation of impairment levels. The FAQ scores may vary across cultures, and diagnostic thresholds may not be transferable across countries [@odonald2025]. This may influence both the sensitivity and ecological validity of the functional criteria used. Another limitation is that the FAQ scores may correlate with neuropsychological results of certain domains, specifically executive functions or memory [@vanrentergem2020; @moheb2017]. This overlap could artificially increase concordance between cognitive and functional criteria. For future research, we suggest using a PD-specific questionnaire, such as the Penn Parkinson's Daily Activities Questionnaire-15 [@brennan2016] or generally more objective methods, preferably performance-based ones. Our results underscore the importance of IADL measurement for the PDD diagnosis. Therefore, we recommend using more reliable tools with high ecological validity.** For future research, we suggest the use of more objective, modernized tools for IADl assessment, such as performance-based methods [@schmitter-edgecombe2020] or questionnaire adaptation including questions regarding gadgets use and digital literacy [@postema2024]. Our results emphasize the importance of IADL deficit for the PDD diagnosis, therefore, we suggest exploring more reliable tools with high ecological validity.

**Additionally, the current study did not explore associations of PDD diagnosis with its neuropsychiatric correlates and biomarkers. We recommend examining models where psychiatric symptoms and biomarkers are treated as covariates to better capture their impact on PDD diagnosis.**

??Finally, whereas our study systematically investigated how varying definition of global deficit, impaired cognition and IADL deficit affect probable PDD classification, it did not explore associations of PDD diagnosis with its neuropsychiatric (e.g. anxiety profile) and biomarker correlates. Future studies would benefit from focusing on these aspects of PDD.??[^2]

[^2]: **JM: Why was the previous paragraph added instead of this one? This looks perfectly functional. The same goes largely for the IADL discussion, the original was better (in terms of resources and logical coherence) at most places.**

# Conclusions

Our study highlights the variability in PDD classification across Level I diagnostic algorithms, heavily influenced by IADL operationalization and the choice of cognitive screening tools. The findings support the call for a change of the current diagnostic criteria [@kulisevsky2024], favouring the use of MoCA-based components and comprehensive IADL assessments.

Conservative criteria, such as reliance on pill questionnaire (i.e. FAQ item 9 equivalence), may fail to detect functional decline and thus under-identify true cases of PDD. Importantly, concordance across algorithms rises significantly, reaching moderate to high values, when the same definition of IADL is used (either FAQ total or FAQ item 9). Moreover, when using MoCA-based algorithms instead of MMSE-based ones, we can observe better approximations to the Level II battery.

# References

# Appendix

## Derivation of the Algorithms Set

Both, the original PDD criteria [@dubois2007] and the call for their change [@kulisevsky2024] allow for several distinct combinations of items to be used to define cognitive impairment. Consequently, in this study we derived all algorithms for probable PDD on Level I that are in line with published criteria. This procedure parallel the diagnostic algorithm outlined in Table 2 of @dubois2007. Specifically, in this study, we varied the exact specification of items 3-5 of this table (i.e., the measure of global cognitive impairment, the measure of the impact on IADLs and the measures of impaired cognition).

For each set of criteria (MMSE-based, MoCA-based, sMoCA-based and Level II), we first specified the items and then the thresholds for each item used to define probable PDD. If more than one option was present in either the choice of the item or the choice of the threshold, we created an algorithm for each choice in turn. The final set of algorithms was arrived at by computing the Cartesian product of all possibilities provided by varying items and thresholds. All combinations are presented in @tbl-algos.

For MMSE-based algorithms, the following sets of items served as the basis:

$Global = \{MMSE < 26\}$

$Attention = \{{Sevens\ backwards < 4}\}$

$Executive = \{{Clock\ drawing} < 2, {Lexical\ fluency\ (S)} < 10\}$

$Construction = \{Pentagons < 1\}$

$Memory = \{{Three{\text-}words\ recall} < 3\}$

$IADL = \{FAQ > 7, {FAQ\ (it. 9)} > 1\}$

The ensuing Cartesian product $Global \times Attention \times Executive \times Construction \times Memory \times IADL$ results in $1 \times 1 \times 2 \times 1 \times 1 \times 2 = 4$ MMSE-based algorithms for probable PDD.

For MoCA-based algorithms, the following sets of items served as the basis:

$Global = \{MoCA < 26\}$

$Attention = \{{Sevens\ backwards < 3}\}$

$Executive = \{{Clock\ drawing} < 2, {Clock\ drawing} < 3, {Lexical\ fluency\ (K)} < 11\}$

$Construction = \{Cube\ drawing < 1\}$

$Memory = \{Five{\text-}words\ recall < 1, Five{\text-}words\ recall < 2, Five{\text-}words\ recall < 3, Five{\text-}words\ recall < 4, Five{\text-}words\ recall < 5\}$

$Language = \{Abstraction < 2, Animal\ naming < 3\}$

$IADL = \{FAQ > 7, {FAQ\ (it. 9)} > 1\}$

Note that the additional language domain adds complexity to establishing a diagnostic algorithm because simply by adding it to the set of items, the number of potential algorithms doubles. Further complexity is added by the fact that there are so far no guidelines for selecting a diagnostic threshold for Clock drawing and Five-words recall tests, both of which differ from their counterparts used by @dubois2007. Finally, although the Sevens backwards item has different thresholds in MoCA-based compared to MMSE-based algorithms, this difference is solely due to a difference in scoring whereby 3 points in MoCA correspond to 4 or 5 points in MMSE. The Seven backwards item threshold for MoCA-based algorithms used in this study is thus equivalent to its MMSE-based counterpart.

Computing the Cartesian product $Global \times Attention \times Executive \times Construction \times Memory \times Language \times IADL$ yields $1 \times 1 \times 3 \times 1 \times 5 \times 2 \times 2 = 60$ distinct MoCA-based algorithms for probable PDD.

For sMoCA-based algorithms, the following sets of items served as the basis:

$Global = \{sMoCA < 13\}$

$IADL = \{FAQ > 7, {FAQ\ (it. 9)} > 1\}$

yielding $Global \times IADL$, i.e., $1 \times 2 = 2$ distinct sMoCA-based algorithms for probable PDD.

Finally, the Level II algorithms were based on the following sets of items:

$Attention = \{z(TMT\ A) < -1.5\ \cup z(WAIS\ DSB) < -1.5\}$

$Executive = \{z(CF\ A) < -1.5\ \cup z(PST\ C) < -1.5\}$

$Construction = \{z(JoLO) < -1.5\ \cup z(CLOXI) < -1.5\}$

$Memory = \{z(RAVLT\ DR) < -1.5\ \cup z(BVMTR\ DR) < -1.5\ \cup z(WMS\text-III\ Family\ Pictures) < -1.5\}$

$Language = \{z(WAIS\ Similarities) < -1.5 \ \cup z(BNT\ 60) < -1.5\}$

$IADL = \{FAQ > 7, {FAQ\ (it. 9)} > 1\}$

where $z()$ denotes calculation of age, sex and education adjusted z-score. This yields $1 \times 1 \times 1 \times 1 \times 1 \times 2 = 2$ distinct Level II algorithms for probable PDD in the current study. All but the BNT 60 item were evaluated using regression norms published by @bezdicek2017. Since the original article used BNT 30 instead of BNT 60, we approximated the deficit in BNT 60 by comparing patients' raw score to age- and education-specific normative values reported by @zemanova2016. Specifically, patients whose BNT 60 score fell below 5^th^ percentile of their demographic group in Table 6 of @zemanova2016 were considered to show signs of impaired performance.

### Operationalization of Impaired Cognition

In the original criteria, item 4 of Level I criteria, i.e., impaired cognition, was defined as follows: *"The proposed diagnostic criteria require a profile of cognitive deficits, typical of those described for PD-D, in two or more of four domains."* [@dubois2007, p. 2316] Consequently, we defined impaired cognition as a deficit in two or more domains of four in MMSE-based criteria and as a deficit in two or more of five domains in MoCA-based criteria. sMoCA-based criteria omitted the "impaired cognition" item altogether because they were intended as a shorter screening alternative to classical Level I assessment. Finally, for the Level II criteria, we employed standard definition of impaired cognition as the *"\[i\]mpairment on at least two neuropsychological tests, represented by either two impaired tests in one cognitive domain or one impaired test in two different cognitive domains."* [@litvan2012, Table 1]

```{r}
#| label: "tbl-algos"
#| tbl-cap: "Summary of all algortihms for probable PDD used in the study."

rates$gtables$algorithms
```

## Theoretical and Empirical Estimands

In this study, we follow the framework proposed by @lundberg2021 for specifying targets of inference (i.e., the estimands) in qunatitative sciences to increase transparency and connect statistical evidence to relevant theory. @tbl-estimands contains verbal description of the components relating to each of our proclaimed research objectives and map them to the population quantity of interest (the theoretical estimand), data-dependent quantity that could be estimated (the empirical estimand) and quantities that are reported in the study (statistical estimates).

```{r}
#| label: "tbl-estimands"
#| tbl-cap: "Mapping between research objectives and quantities of interest in the current study."

table_estimands()
```

The RO1 - to estimate the PDD rate and evaluate the diagnostic variability and concordance across different algorithms of probable PDD - was divided into four distinct research objectives:

-   to estimate the rate of PDD within PD (RO1.1),
-   to estimated variability of this rate (RO1.2),
-   to evaluate predictive value of demographic variables for probable PDD classification (RO1.3) and
-   to evaluate concordance between different probable PDD operationalizations and criteria (RO1.4).

Estimates relating to RO1.1 and RO1.3 cannot be safely generalized beyond a population of PD patients that are candidates for DBS due to the systematic differences between DBS candidates pool and general PD population (such as the lower age of DBS candidates compared to the general PD population). On the other hand, the estimates relating to RO1.4 (and to a lesser degree to RO1.2[^3]) may not be substantially influenced by the sample at hand as the primary source of their variance might come from variability in measures employed (e.g., MMSE vs MoCA to assess global cognitive performance) rather than variability in patients' performance. Assuming that there is no substantial Differential Item Functioning for DBS candidates compared to a broader population of patients with PD, the estimates relating to RO1.4 can be cautiously generalized beyond the current sample.

[^3]: Because the quantity of interest is a rate and could thus be though of as a sum of binomially distributed PDD occurences divided by the total number of patients, its variance will likely systematically vary with its mean. Specifically, as the rate goes from extremes to 0.5, the variance increases. Consequently, if our estimate of the rate was lower than the true population rate, e.g., because our sample includes younger patients compared to the general PD population, our estimate of variance would also be lower than the true variance of PDD rate in the general PD population. Nonetheless, the between-algorithm variability may not be affected by this phenomenon as unlike variability of PDD rate, we do not have reason to assume it comes about by summing independent binomial events.

Finally, for the RO2, the theoretical estimand is defined as the set of diagnostic components whose variation systematically alters the probability of a probable PDD diagnosis. This aspect of the study is exploratory in nature. Empirically, we assess the contribution of each diagnostic feature by examining how variations in operational definitions (e.g., domain-specific thresholds, criteria for functional impairment) influence the statistical estimates derived for the first objective. This allows us to identify the diagnostic elements most responsible for between-algorithm discrepancies.

## Supplementary Presentation of Results

```{r}
#| label: "fig-rates"
#| fig-cap: "Summary of the estimates of probable PDD rate."
#| apa-note: "Vertical lines represent estimates arrived at by using sMoCA (dotted) or Level II (dashed) with FAQ item 9 (orange) or FAQ total score (blue) as criteria for probable PDD."

rates$plot
```

```{r}
#| label: "tbl-rates"
#| tbl-cap: "Estimates of the rate of probable PDD in the sample."

rates$gtables$rates
```

```{r}
#| label: "tbl-approx-bal"
#| tbl-cap: "Level I algorithms for probable PDD as predictors of Level II classification as the reference arranged by their balanced accuracy score."

table_levelII_approximations(conc$table, algos, "Balanced Accuracy")
```

```{r}
#| label: "fig-data"
#| fig-cap: "Representation of study data."
#| apa-note: "The figure shows whether patients (x-axis) ordered from the youngest (left) to the oldest (right) were classified as probable PDD by each tested algorithm (y-axis) ordered from the one with the lowest (bottom) to the highest (top) PDD rate estimate. Patients printed in red are women, patients printed in blue are men. Red cells indicate probable PDD diagnosis, grey cells indicate non-PDD diagnosis and white cells indicate missing diagnosis."

pred$plots$data
```

```{r}
#| label: "fig-pars"
#| fig-cap: "Summary of logistic regressions parameters prediction probable PDD by age and sex."
#| apa-note: "Histograms represent odd ratio (OR) estimates and p-values associated with age, sex, and their interaction as predictors of each of the 68 probable PDD classification. In the case of parameters for sex, values higher than 20 were omitted for clarity. Vertical lines indicate OR = 1 and p = .05."

pred$plots$parameters
```

```{r}
#| label: "fig-kappa"
#| fig-cap: "Cohen's κ matrix."
#| apa-note: "The matrix depicts Cohen's κ measuring agreement between algorithms for PDD. Algorithms printed in red defined IADL deficit by FAQ total score, algorithms printed in black defined IADL deficit by FAQ item 9 response."

conc$plots$Kappa
```

```{r}
#| label: "fig-sens"
#| fig-cap: "Sensitivity matrix."
#| apa-note: "The matrix depicts sensitivity of algorithms for PDD depicted on x-axis in predicting outcomes based on algorithms on the y-axis. Algorithms printed in blue defined IADL deficit by FAQ total score, algorithms printed in black defined IADL deficit by FAQ item 9 response."

conc$plots$Sensitivity
```

```{r}
#| label: "fig-spec"
#| fig-cap: "Specificity matrix."
#| apa-note: "The matrix depicts specificity of algorithms for PDD depicted on x-axis in predicting outcomes based on algorithms on the y-axis. Algorithms printed in blue defined IADL deficit by FAQ total score, algorithms printed in black defined IADL deficit by FAQ item 9 response."

conc$plots$Specificity
```
