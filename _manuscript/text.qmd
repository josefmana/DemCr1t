---
lang: en
title: "Updated Criteria for the Diagnostic Procedure for Parkinson’s Disease Dementia on Level I and their Validity in Deep Brain Stimulation Cohort"
shorttitle: "Parkinson's Disease Dementia Level I Criteria"
author:
  - name: Martina Mana
    corresponding: false
    orcid: "0009-0007-4665-3946"
    role:
      - Conceptualization
      - Data curation
      - Writing - original draft
    affiliations:
      - id: "1FM"
        name: "First Faculty of Medicine and General University Hospital in Prague, Charles University, Czech Republic"
        department: "Department of Neurology and Centre of Clinical Neuroscience"
  - name: Josef Mana
    corresponding: false
    orcid: "0000-0002-7817-3978" 
    email: "josef.mana@lf1.cuni.cz"
    role:
      - Conceptualization
      - Data curation
      - Investigation
      - Formal analysis
      - Software
      - Methodology
      - Project administration
      - Validation
      - Writing - original draft
    affiliations:
      - id: "1FM"
        name: "First Faculty of Medicine and General University Hospital in Prague, Charles University, Czech Republic"
        department: "Department of Neurology and Centre of Clinical Neuroscience"
  - name: Tereza Uhrova
    corresponding: false
    email: "tereza.uhrova@vfn.cz"
    role:
      - Investigation
    affiliations:
      - id: "1FM"
        name: "First Faculty of Medicine and General University Hospital in Prague, Charles University, Czech Republic"
        department: "Department of Neurology and Centre of Clinical Neuroscience"
  - name: Robert Jech
    corresponding: false
    orcid: "0000-0002-9732-8947"
    email: "jech@cesnet.cz"
    role:
      - Funding acquisition
      - Resources
      - Writing - review & editing
    affiliations:
      - id: "1FM"
        name: "First Faculty of Medicine and General University Hospital in Prague, Charles University, Czech Republic"
        department: "Department of Neurology and Centre of Clinical Neuroscience"
  - name: Ondrej Bezdicek
    corresponding: true
    orcid: "0000-0002-5108-0181"
    email: "ondrej.bezdicek@gmail.com"
    role:
      - Investigation
      - Data curation
      - Funding acquisition
      - Conceptualization
      - Project administration
      - Supervision
      - Writing - original draft
    affiliations:
      - id: "1FM"
        name: "First Faculty of Medicine and General University Hospital in Prague, Charles University, Czech Republic"
        department: "Department of Neurology and Centre of Clinical Neuroscience"
abstract: ""
keywords:
  - Parkinson’s disease
  - Parkinson's disease dementia
  - level I criteria
format:
 apaquarto-pdf:
    # Can be jou (journal), man (manuscript), stu (student), or doc (document)
    documentmode: man
    keep-tex: true
 apaquarto-docx: 
    toc: false
bibliography: bibliography.bib
floatsintext: false
numbered-lines: false
suppress-title-page: false
warning: false
echo: false
---

```{r}
#| label: "import"

library(targets)
library(tidyverse)
library(gt)

upstore <- here::here(tar_config_get("store"))
tar_source(here::here("R"))
data <- tar_read(raw_data, store = upstore)
crit <- tar_read(pdd_data, store = upstore)$criteria
vars <- tar_read(variables, store = upstore)
```

# Introduction

Parkinson’s disease (PD) is a neurodegenerative disorder defined by a gradual onset of motor impairment, including rigidity, bradykinesia, postural instability and resting tremor. Additionally, patients frequently suffer from a range of non-motor impairments [@postuma2015], particularly cognitive decline. This state might result in Parkinson’s disease dementia (PDD) in a subset of patients [@meireles2012].

Despite its clinical relevance, the diagnosis of PDD remains complex. The original criteria formalized in 2007 by the International Parkinson and Movement Disorder Society (MDS) [@dubois2007] were heavily influenced by frameworks established primarily for Alzheimer’s disease (AD). **Although these criteria provided a valuable base, they lacked the specificity to detect the specific cognitive aspects of PDD [@emre2007; @yamashita2023].**[^1]

[^1]: **JM: Are we sure the references provide evidence supporting our statement that MDS criteria lacked specificity for cognitive aspects of PDD? Emre et al. (2007) is one of the series of PDD-criteria-creating articles together with Dubois et al. (2007) and it does not say the criteria are not specific. Yamashita et al. (2023) is not concerned with criteria at all, instead they use any definition of PDD that was present in their source studies to evaluate predictive value of biomarkers for PDD. I do not think our argument here is valid based on the provided evidence.**

In order to increase diagnostic accuracy, the MDS also developed a levelled system for PDD detection. Level I criteria are designed primarily for clinical use, consisting of brief cognitive assessments **and relying on clinical judgement**[^2]. In contrast, Level II criteria use comprehensive neuropsychological testing across cognitive domains to provide greater diagnostic specificity [@emre2007].

[^2]: **JM: Is this a statement directly from the source? If not, I would drop the clinical judgement part as all diagnoses are likely to rely on clinical judgement anyway.**

One specific feature of the original Level I criteria was the presence of an algorithm allowing flexibility in test selection. The criteria included (Mini-Mental State Examination) MMSE pentagons for evaluation of visuospatial ability, three-word recall for memory assessment, months reversed or seven backwards for attention assessment and lexical fluency or clock drawing for executive function assessment [@dubois2007].[^3]

[^3]: **JM: I would introduce the algorithm in a little bit of more detail. E.g., "The original criteria included a straightforward algorithm for diagnosis of probable PDD consisting of eight items that needed to be fullfilled simultaneously. ...**

According to a recent meta-nalysis, the estimates of PDD rate within PD vary widely, ranging from 14% up to 55%, depending on methodological criteria employed [@sousa2022]. Moreover, factors such as patients\` sex [@cereda2016], age and disease duration appear to modulate the risk of cognitive decline [@oh2016; @rana2012]. Currently, efforts are focused on refining the PDD diagnostic framework to improve its consistency and applicability in both research and clinical contexts [@kulisevsky2024].[^4]

[^4]: **JM: This shall be even higher, the second paragraphs is a good place for this one according to my humble correct opinion.**

**The present study evaluates the diagnostic concordance**[^5] between the original Level I criteria for PDD, as established by the MDS Task Force [@emre2007; @dubois2007] and criteria inspired by the recent call for change [@kulisevsky2024]. Furthermore, both sets of criteria are compared to PDD diagnosed on Level II. The study aims to address the following research objectives (RO): (RO1) To estimate the Parkinson’s disease dementia (PDD) rate and evaluate the diagnostic variability and concordance across different PDD criteria. (RO2) To identify specific diagnostic components contributing to PDD classification variability across criteria.

[^5]: **JM: There is not much reasoning substantiating these goals in the introduction. We would be better off restructuring the introduction with a more straightforward logic that goes: "This is PDD" -\> "PDD is important" -\> "This is how PDD is measured" -\> "This is why the PDD measurement is imperfect" -\> "This is what can help us to make sense of the PDD measurement" -\> "Ergo, that's why we do what we do in this study."**

# Methods

## Participants

```{r}
#| label: dates

continue <- all(!is.na(data$assdate))
stopifnot("Some assessment dates are missing" = continue)

dates <-
  sapply(
    c("min", "max"),
    function(f) {
      date <- do.call(f, list(data$assdate))
      paste(month(date, label = T, abbr = F), year(date))
    }
  )
```

This study retrospectively analyzed clinical data from a cohort of patients with PD at the General University Hospital in Prague. All patients were diagnosed with idiopathic PD by a movement disorder specialist according to the MDS Clinical Diagnostic Criteria for PD [@postuma2015]. Clinical records spanning `r dates["min"]` to `r dates["max"]` were examined. All participants underwent neuropsychological evaluation conducted by a trained clinical psychologist (OB) as part of standard preoperative cognitive assessments for DBS eligibility at the General University Hospital in Prague.

Ethical approval for the study protocol was obtained from the Ethics Committee of the General University Hospital in Prague. Informed consent was secured from all patients prior to their neuropsychological assessments, in adherence to ethical research guidelines.

## Neuropsychological Assessment

Cognitive performance was evaluated at both Level I (abbreviated assessment) and Level II (comprehensive assessment) according to the standard MDS neuropsychology battery for Parkinson's Disease Mild Cognitive Impairment (PD-MCI) [@litvan2012, @bezdicek2017]. Level I was assessed using the Mini-Mental State Examination (MMSE) [@stepankova2014; @folstein1975] and the Montreal Cognitive Assessment (MoCA) [@kopecek2016; @nasreddine2005]. The neuropsychological assessment at Level II covered five cognitive domains, each evaluated through specific tests as follows: attention and working memory assessed using Trail Making Test Part A (TMT-A) [@bezdicek2012; @reitan2004], and WAIS Digit Span Backward (WAIS DSB) [@wechsler1997]; executive function evaluated via Categorical Verbal Fluency (CF) [@nikolai2015], and subtest from the Prague Stroop Test – Colors (PST-C) [@bezdicek2021]; language measured with the WAIS Similarities subtest [@wechsler1997], and the Boston Naming Test (BNT-60) [@zemanova2016; @kaplan1983]; memory examined using the Rey Auditory Verbal Learning Test (RAVLT) [@frydrychova2018; @bezdicek2013; @rey1964] delayed recall, and the Brief Visual Memory Test–Revised (BVMTR) [@havlik2020; @benedict1997] delayed recall, or WAIS Family Pictures subtest [@wechsler1997] delayed recall; visuospatial function assessed through the Judgment of Line Orientation Test (JoLO) [@benton1983], and Clock Drawing Test (CLOX) [@royall1998].

To assess functional impairment, the Functional Activities Questionnaire (FAQ) [@bezdicek2016; @pfeffer1982] was administered. Additionally, neuropsychiatric status was evaluated using the Beck Depression Inventory-II (BDI-II) [@ciharova2020; @beck1996] and State-Trait Anxiety Inventory (STAI) [@spielberg1983; @mullner1980]. Psychotic symptoms were assessed through structured psychiatric interviews conducted by a trained psychiatrist.

## Diagnostic algorithms for probable Parkinson's Disease Dementia

In this study, we applied three distinct sets of diagnostic algorithms for probable PDD at Level I. The first set was based on the original framework [@dubois2007], which utilized the Mini-Mental State Examination (MMSE) as a global cognitive screening tool, supplemented by assessments of attention, executive function, visuospatial abilities, and memory. The second set of algorithms was based on the recent call for change of dementia diagnostic guidelines [@kulisevsky2024], which advocates for more sensitive cognitive domain assessments in the context of PD. This updated approach incorporated specific items from the Montreal Cognitive Assessment (MoCA). The third approach applied the Czech version of the shortened Montreal Cognitive Assessment (sMoCA) [@bezdicek2020], a time-efficient modification designed to measure global cognitive performance using a reduced testing protocol that omits items providing redundant information. Lastly, the fourth approach followed the Level II protocol for diagnosis of PDD and Mild Cognitive Impairment in PD (PD-MCI) [@dubois2007; @litvan2012]. The Level II methodology, including the use of a regression-based normative scoring approach, has been detailed in a prior study [@bezdicek2017]. In this study, the thresholds for cognitive impairment at Level II were set at $z \leq -1.5$. All non-cognitive criteria of probable PDD (i.e., diagnosis of PD that developed before dementia and absence of Major Depression, delirium or other abnormalities that obscure diagnosis) held true for all patients in the sample according to the psychiatric and neurological examinations.

```{r}
#| label: crit-check

critnum <-
  sapply(
    c("mmse", "moca", "smoca", "lvlII"),
    function(i)
      subset(crit, group == i) |>
      nrow()
  )

continue <- !any(critnum == 0)
stopifnot("Some criteria are missing" = continue)
```

For each of these diagnostic approaches, we applied two operationalizations of deficits in Instrumental Activities of Daily Living (IADL). First, we utilized FAQ item 9, which approximates the pill questionnaire from the original criteria [@dubois2007] employing a cut-off score of 2 points or higher. Second, we applied the entire Functional Activities Questionnaire (FAQ) as suggested in the call for change [@kulisevsky2024], employing a cut-off score of 7 points or higher based on Czech normative data [@bezdicek2011]. These methodologies resulted in a total of `r sum(critnum)` algorithms, which were distributed across different diagnostic criteria: `r critnum["mmse"]` MMSE-based, `r critnum["moca"]` MoCA-based, `r critnum["smoca"]` sMoCA-based, and `r critnum["lvlII"]` based on the Level II battery (see @tbl-crits and Appendix @tbl-algos for the exact specification of each algorithm).

```{r}
#| label: tbl-crits
#| tbl-cap: "Summary of probable PDD operationalizations compared in the study."

table_criteria(vars)
```

\[Insert Table 1 here\]

## Statistical Analyses

Following the framework proposed by @lundberg2021, in this study we explicitly connect our research objectives and their corresponding theoretical (i.e., targets of inference) and empirical (i.e., data-driven) estimands to statistical estimates. The theoretical estimand refers to a unit-specific quantity defined over a target population and represents the ideal quantity that would address the research question under optimal conditions, such as access to complete population data or perfect experimental control. In contrast, the empirical estimand corresponds to the quantity that is actually computable using the available dataset, given real world constraints. The full description of the study's estimands and their relation to our research objectives is presented in the Appendix (see @tbl-estimands).

To address study objectives, we started by repeatedly assigning each patient the diagnosis of probable PDD based on each PDD algorithm listed in @tbl-crits (see also @tbl-algos) resulting in a `r sum(data$incl)` (patients) $\times$ `r sum(critnum)` (algorithms) matrix where each cell indicates whether a patient (row) meets criteria for probable PDD according to an algorithm (column)[^6]. PDD rate estimates were computed as $\frac{N_{PDD}}{N_{total}}$ separately for each algortihm. The predictive value of age and sex was then evaluated by fitting a set of logistic regressions, one for each algorithm for probable PDD, whereby the probable PDD was predicted by age, sex and their interaction.

[^6]: **JM: This could and should be shared most likely, as long, as we anonymize properly. Let's ask Oto Mestek and the NPO team how and if is it possible.**

Next, a set of two class cross-tabulations with associated statistics was computed for each pair of algorithms via the `confusionMatrix()` function from the R package *caret* [@kuhn2008]. For each pair of algorithms, the analysis was repeated twice such that each variable of the pair served once as the reference and once as the predictor. Following measures were used to evaluate pairwise concordance between different algorithms for probable PDD: 1) Cohen's $\kappa$ with its 95% confidence interval (CI) computed via the `cohen.kappa()` function from the R package *psych* [@revelle2024]; 2) Accuracy (i.e., the proportion of correct predictions, both true positives and true negatives, among the total number of cases) with its 95% CI; 3) Sensitivity/Recall (i.e., the proportion of true positives); and 4) Specificity (i.e., the proportion of true negatives).

Finally, the No Information Rate (NIR) was calculated for each pair of algorithms. NIR is the accuracy that could be obtained by always predicting the majority class and in our case it is equivalent to the complement of the PDD rate estimate according to the reference algorithm. Accuracy of prediction was compared to the NIR via a one-sided Exact Binomial Test as implemented by the `binom.test()` R stats function. Reference/predictor pairs associated with p \< .05 were considered to show significantly better accuracy than NIR. In other words, for reference/predictor pairs associated with p \< .05, we conclude that knowing the probable PDD status according to the predictor algorithm helps to estimate the probable PDD status according to the reference algorithm and the two algorithms thus show substantial concordance.

Data wrangling and visualizations were done in the *tidyverse* package [@wickham2019] and tables were formatted in the *gt* package [@iannone2024]. All analyses were conducted within the R (version `r with(version, paste(major, minor, sep="."))`) software environment for statistical computing [@rsoft]. The software code supporting this article is available at <https://github.com/josefmana/DemCr1t.git>.[^7]

[^7]: **JM: Do not forget to make it public before submitted!**

# Results

```{r}
#| label: results

desc <- tar_read(sample_description, store = upstore)
prev <- tar_read(prevalence_summaries, store = upstore)
pred <- tar_read(demographic_predictors, store = upstore)
conc <- tar_read(concordance_statistics, store = upstore)
perc <- prev$table$perc

# In-text demographic data
sex <- subset(desc$table, variable == "Sex")$N
age <- paste0(subset(desc$table, variable == "Age")[ , c("M", "SD")] |> paste(collapse = " (SD = "),") years of age")
edu <- paste0(subset(desc$table, variable == "Education")[ , c("M", "SD")] |> paste(collapse = " (SD = ")," ) years of education")
dur <- paste0(subset(desc$table, variable == "PD duration")[ , c("M", "SD")] |> paste(collapse = " (SD = "),") years of disease duration")
updrs_off <- paste0(subset(desc$table, variable == "UPDRS III off state")[ , c("M", "SD")] |> paste(collapse = " (SD = "),") Unified Parkinson Disease Rating Scale (UPDRS), part III in medication OFF state")
updrs_on <- paste0(subset(desc$table, variable == "UPDRS III on state")[ , c("M", "SD")] |> paste(collapse = " (SD = "),") UPDRS III in medication ON state")

# In-text rate estimates
rates_overall <- sapply(c("M", "SD", "Md", "minmax"), function(f) do_summary(perc, 2, f))
rates_FAQ9 <- sapply(c("M", "SD", "Md", "minmax"), function(f) do_summary(perc[grepl("FAQ 9", prev$table$IADL)], 2, f))
rates_FAQtot <- sapply(c("M", "SD", "Md", "minmax"), function(f) do_summary(perc[prev$table$IADL == "FAQ > 7"], 2, f))

# Minimum p-value of logistic regressions
min_p <- subset(pred$values, quantity == "p-value")$estimate |> min() |> do_summary(3, "p")
```

## Sample Description

A total of `r sum(data$incl)` patients were included. The sample included `r sex` men, with an average of `r age`, `r edu`, `r dur`, `r updrs_off` and `r updrs_on`. Cognitive characteristics of the sample are summarized in @tbl-desc.

```{r}
#| label: tbl-desc
#| tbl-cap: "Cognitive characteristics of the sample."

desc$gtable
```

\[Insert Table 2 here\]

## PDD Rate Estimates

Algorithm-wise rate of PDD estimates are presented in @tbl-rates. On average, estimated PDD rate was `r rates_overall["M"]`% (SD = `r rates_overall["SD"]`, Md = `r rates_overall["Md"]`, range `r rates_overall["minmax"]`). Notably, the estimates were substantially lower when FAQ item 9 was used as a criterion of IADL deficit (M = `r rates_FAQ9["M"]`% SD = `r rates_FAQ9["SD"]`, Md = `r rates_FAQ9["Md"]`, range `r rates_FAQ9["minmax"]`) compared to using total FAQ score criterion (M = `r rates_FAQtot["M"]`% SD = `r rates_FAQtot["SD"]`, Md = `r rates_FAQtot["Md"]`, range `r rates_FAQtot["minmax"]`) as demonstrated also in @fig-rates. Neither age nor sex or their interaction (*p*s $\geq$ `r min_p`) reliably predicted probable PDD classification across algorithms (see @fig-data and @fig-pars).

\[Insert Figure 1 here\]

```{r}
#| label: fig-rates
#| fig-cap: Summary of the estimates of probable PDD rate.
#| apa-note: Vertical lines represent estimates arrived at by using sMoCA (dotted) or Level II (dashed) with FAQ item 9 (orange) or FAQ total score (blue) as criteria for probable PDD.

prev$plot
```

## Concordance between Algorithms

```{r}
#| label: kappas

algos <- list(
  faq_tot = subset(prev$table, IADL == "FAQ > 7")$type,
  faq_9 = subset(prev$table, IADL == "FAQ 9 > 1")$type
)

# Check for overlap:
continue <- !(any(algos$faq_tot %in% algos$faq_9) || any(algos$faq_9 %in% algos$faq_tot))
stopifnot("Some algorithm(s) use both IADL operationalisations!" = continue)

# Prepare combinations within IADL operationalisation:
kappas <-
  lapply(
    set_names(names(algos)),
    function(o)
      combn(algos[[o]], 2) |>
      t() |>
      as_tibble() |>
      `colnames<-`(c("predictor", "reference")) |>
      left_join(conc$table, by = c("predictor", "reference")) |>
      select(Kappa_raw) |>
      pull()
  )

# Add cross-operationalisation cases
kappas$cross <- subset(conc$table, predictor %in% algos$faq_9 & reference %in% algos$faq_tot)$Kappa_raw

# Extract Kappa summaries:
kappa_sums <-
  sapply(
    names(kappas),
    function(i)
      paste0(do_summary(kappas[[i]], 2, "M"),", SD = ", do_summary(kappas[[i]], 2, "SD"))
  )
```

Results of the analyses of prediction Accuracy, Cohen's $\kappa$, Sensitivity and Specificity are presented in @fig-acc, @fig-kappa, @fig-sens and @fig-spec respectively. **Numerical results are available at ...** [^8]. Generally, algorithms that employed the same operationalization of IADL deficit showed substantial pairwise concordance, however, algorithms that operationalized IADL deficit differently did not. Whereas among algorithms with identical IADL deficit operationalization, the agreement judged by Cohen's $\kappa$ was moderately high (operationalization by FAQ total score: $\kappa$ = `r kappa_sums["faq_tot"]`; operationalization by FAQ item 9: $\kappa$ = `r kappa_sums["faq_9"]`), among algorithms that differ in IADL deficit operationalization it was low ($\kappa$ = `r kappa_sums["cross"]`).

[^8]: **JM: Should provide an excel sheet or html table the readers could go through by themselves.**

\[Insert Figure 2 here\]

```{r}
#| label: fig-acc
#| fig-cap: Prediction accuracy matrix.
#| apa-note: The matrix depicts classification accuracy of algorithms for PDD depicted on x-axis in predicting outcomes based on algorithms on the y-axis. Algorithms printed in blue defined IADL deficit by FAQ total score, algorithms printed in black defined IADL deficit by FAQ item 9 response. Cases with asterisk indicate predictive accuracy statistically significantly higher than the No Information Rate.

conc$plots$Accuracy
```

## Prediction of Level II Criteria

```{r}
#| label: lvlII-stats

lvlII_rates <- c(
  faq_tot = subset(prev$table, type == "Lvl.II (1)")$perc |> do_summary(2) |> paste0("%"),
  faq_9 = subset(prev$table, type == "Lvl.II (2)")$perc |> do_summary(2) |> paste0("%")
)

smoca_stats <- c(
  spec = subset(conc$table, predictor == "sMoCA (1)" & reference == "Lvl.II (1)")$Specificity |> do_summary(2),
  sens = subset(conc$table, predictor == "sMoCA (1)" & reference == "Lvl.II (1)")$Sensitivity |> do_summary(2)
)
```

For easier interpretability of our results, we next examine cases where Level II algorithms served as a reference and Level I algorithms as a predictor. @tbl-approx shows five Level I algorithms with the highest and five with the lowest accuracy in predicting Level II classification of probable PDD.

When IADL deficit was defined by total FAQ score, the Level II estimate of PDD rate was `r lvlII_rates["faq_tot"]`. All five Level I algorithms that approximated the Level II classification most accurately were MoCA-based and defined Executive Function deficit by Clock drawing rather than Lexical fluency test. On the other hand, two out of the five Level I algorithms with the lowest accuracy were MMSE-based, whereas the remaining three were MoCA-based and defined Executive Function deficit by Lexical fluency test.

When IADL deficit was defined by FAQ item 9 score, the Level II estimate of PDD rate was `r lvlII_rates["faq_9"]`. Overall, the difference between the most accurate and the least accurate Level I algorithms was lower than in the case of IADL deficit being defined by FAQ total score (see @tbl-approx). The five most accurate algorithms were all MoCA-based, defined Executive Function deficit by Clock drawing (with threshold \< 2) and in majority of cases defined Language deficit by Animal naming. Two out of the five Level I algorithms with the lowest accuracy were MMSE-based, whereas the remaining three were MoCA-based and defined Executive Function deficit by Clock drawing (with threshold \< 3) and Language deficit by Abstraction.

Finally, if the predictors are sorted by their balanced accuracy (i.e., average of sensitivity and specificity) instead of raw accuracy, the results are similar with the exception that for prediction of Level II with total FAQ score algorithm for probable PDD, the highest balanced accuracy was achieved by the sMoCA algorithm with sensitivity `r smoca_stats["sens"]` and specificity `r smoca_stats["spec"]` (see @tbl-approx-bal).

```{r}
#| label: tbl-approx
#| tbl-cap: "Level I algorithms for probable PDD as predictors of Level II classification as the reference."

table_levelII_approximations(conc$table, algos, "Accuracy_raw")
```

# Discussion

This study examined the diagnostic variability and validity of Level I criteria for PDD, **specifically in candidates for DBS treatment**[^9]. By systematically comparing 68 diagnostic algorithms based on MMSE, MoCA, sMoCA, and Level II criteria, our findings highlight the significance of algorithm design.

[^9]: **JM: All-right, I do not think it needs to be here either.**

## Variability in PDD Rate Estimates

Our results showed a wide range in estimated PDD rate across algorithms, ranging from 2.00% to 16.75%. Estimates reached lower rates when using solely FAQ item 9 (as an approximation of the pill questionnaire suggested by @dubois2007) in comparison with the full FAQ scale. This discrepancy highlights the diagnostic importance of IADL assessment. Our overall PDD rates were consistently lower compared to previous studies regarding PDD among PD patients, **demonstrating wide variability based on various criteria used**[^10]. For instance, a retrospective study reported a PDD rate of 19.7% [@rana2012], while other clinical investigation found even higher rates, reaching up to 30% [@aarsland2005][^11]. A recent complex meta-analysis synthesizing global data placed the expected PDD rate in PD at 26.30% [@sousa2022]. **The discrepancy is likely attributable to our DBS-specific cohort, which differs in important characteristics, such as the younger age of the patients [@rana2012]. / Compared to these estimates, our study reports generally lower PDD rates, likely reflecting differences in sample characteristics, diagnostic criteria, and methodology.** [^12] [^13]

[^10]: **JM: What is demonstrating this variability? Our findings or previous ones?**

[^11]: **JM: There was aarsland2011 originally but I did not find such a study. Is this reference correct?**

[^12]: **Because the quantity of interest is a rate and could thus be though of as a sum of binomially distributed PDD occurences divided by the total number of patients, its variance will likely systematically vary with its mean. Specifically, as the rate goes from extremes to 0.5, the variance increases. Consequently, if our estimate of the rate was lower than the true population rate, e.g., because our sample includes younger patients compared to the general PD population, our estimate of variance would also lower than the true variance of PDD rate in the general PD population. Nonetheless, the between-algorithm variability may not be affected by this phenomenon as unlike variability of PDD rate, we do not have reason to assume it comes about by summing independent binomial events**

[^13]: \*\*JM: I prefer the second one with added sentence like: "Specifically, our cohort included candidates for DBS who are generally younger than the general PD population and age was repeatedly shown to be a strong predictor of PDD (some citations)." We should also incorporate here a statement about missing evidence of age being predictor in our sample.\*

## Concordance Between Diagnostic Algorithms

Our pairwise comparisons of diagnostic algorithms showed that agreement was notably stronger among those using the same IADL operationalization **(**$\kappa$ range from 0.75, SD = 0.13 to 0.86, SD = 0.09)[^14] compared to those using different IADL definitions **(**$\kappa$ = 0.34, SD = 0.08)[^15]. **Moreover, the agreement was slightly higher between algorithms that defined IADL deficit by FAQ item 9 compared to algorithms that defined it using the full FAQ scale. One possible explanation of this difference follows from the observation that algorithms using the full scale definition yielded higher PDD rate estimates. Because there was higher probability of being diagnosed with IADL deficit based on the full FAQ scale, there was also a bigger room for disagreement in the Cognitive Impairment status when different indexes weer used (e.g., by defining executive deficit via clock drawing vs. lexical fluency).**

[^14]: **JM: There is no need to repeat these numbers in this section.**

[^15]: **JM: Same as above.**

**Overall, algorithms using the same definition of IADL deficit showed degree of concordance equivalent to moderate (when using FAQ total score to define IADL deficit) or strong (when using FAQ item 9 to define IADL deficit) level of agreement between raters in an inter-rater reliability analysis [@mchugh2012]. On the other hand, the concordance between algorithms using different IADL deficit definitions was equivalent to minimal agreement.** This demonstrates that even slight methodological differences can yield divergent diagnostic outcomes. Such findings are critical for clinicians relying on Level I criteria for eligibility decisions, as the choice of algorithm could lead to contradictory classifications of PDD.

## Predictive Validity Comparison With Level II Criteria

When using Level II diagnosis as the reference, MoCA-based Level I algorithms, particularly those using Clock Drawing to assess executive function, demonstrated the highest predictive accuracy. This supports recent proposals to modernize PDD diagnostic frameworks [@kulisevsky2024], favoring MoCA-derived components and broader functional assessment tools. In contrast, MMSE-based algorithms consistently underperformed, suggesting limited sensitivity in capturing the nuanced cognitive deficits characteristic of PDD.

The advantage of MoCA-based algorithm was evident when using the FAQ total score to define functional impairment in comparison with using solely FAQ item 9. In these cases, the best-performing algorithms achieved accuracy rates of 94–99%, together with high sensitivity and specificity. **JM: Some sMoCA praise should be added here ...**

## Clinical Implications^[**JM: Is this section needed? It does not seem to say much to me. I would't be afraid to drop it.**]

Our findings have immediate implications for DBS candidacy assessment. Given that a diagnosis of PDD typically excludes surgical intervention, inconsistent application of diagnostic criteria can lead to unbalanced access to the treatment.

Moreover, the relative success of multidomain assessments, such as Level II protocols, highlights the importance of psychometric precision. This approach may serve as a confirmatory tool in cases where Level I assessments provide ambiguous results.

## Limitations and Future Directions

This study’s generalizability is limited by its retrospective design and the homogeneity of the patient cohort, which may not reflect broader PD populations with varying cognitive profiles. **Specifically, younger age of the sample and possible lack of patients with high risk phenotypes for PDD compared to general population of people with PD might have been responsible for comparatively lower PDD rate estimates in our study. Nonetheless, the analysis of the concordance between different algorithms might not be affected by the sample to the same degree allowing for a broader generalization. Future studies could replicate our results on different cohorts of PD patients. To make this process easier, the code used to generate our results is publicly available and easily applicable to other data sets with a similar data structure.**

**JM: The fact that no biomarkers were used and the IADL was at best estimated by FAQ which is somewhat archaic (comp. to the e-IADL thing which could be also cited and is more in line with Kulisevsky) should be mentioned. Also, this is a great space for an advert for NOT/DOT because the self-report of IADLs can be viewed as a limitation worth of addressing, especially since our results imply possible central role for IADL deficit definition in further work on PDD definition to achieve good reliability.**

Future research should focus on the validation of updated Level I criteria using longitudinal data and neurobiological markers. Furthermore, the development of adaptive diagnostic algorithms that can combine performance across cognitive and functional domains may enhance diagnostic sensitivity while reducing the risk of **over-classification of PDD**^[**JM: What's an over-classification of PDD?**].

# Conclusions

This study systematically investigated the application of multiple Level I diagnostic criteria for PDD within a cohort of patients considered for DBS treatment. Our results show variability in PDD rate estimates, strongly influenced by the choice of cognitive screening instruments and the operationalization of functional impairment. The divergence observed across algorithms demonstrates the sensitivity of diagnostic outcomes to seemingly negligible methodological choices.^[**JM: This sounds as a great opening to the discussion, rather than conclusions :-)**]

The **proposed changes**^[**JM: They did not propose anything though and from what I gather, the authors stress this a lot - that their article in not a proposal of new criteria, only a position paper.**] of the current diagnostic criteria [@kulisevsky2024] offer heightened sensitivity by proposing the use of MoCA-based components and broader IADL assessments. Conservative criteria, such as reliance on pill questionnaire (i.e. FAQ item 9 equivalence), may fail to detect meaningful functional decline and thus under-identify true cases of PDD. **On the other hand, such criteria show slightly higher agreement between their different forms.**

Conclusively, this study contributes to the improvement of PDD diagnostics by offering **evidence for the clarification of Level I criteria**^[**JM: What is meant by clarification of Level I criteria? Also, how does our study offer evidence for such thing?**] and **emphasizing the value of psychometric precision in clinical neuropsychology**^[**JM: This is independent of our study.**]. Future research should validate the algorithms longitudinally and explore possible integration of biomarkers to further improve diagnostic reliability.

# References

# Appendix

## Derivation of the Algorithms Set

Both, the original PDD criteria [@dubois2007] and the call for their change [@kulisevsky2024] allow for several distinct combinations of items to be used to define cognitive impairment. Consequently, in this study we derived all algorithms for probable PDD on Level I that are in line with published criteria. This procedure parallel the diagnostic algorithm outlined in Table 2 of @dubois2007. Specifically, in this study, we varied the exact specification of items 3-5 of this table (i.e., the measure of global cognitive impairment, the measure of the impact on IADLs and the measures of impaired cognition).

For each set of criteria (MMSE-based, MoCA-based, sMoCA-based and Level II), we first specified the items and then the thresholds for each item used to define probable PDD. If more than one option was present in either the choice of the item or the choice of the threshold, we created an algorithm for each choice in turn. The final set of algorithms was arrived at by computing the Cartesian product of all possibilities provided by varying items and thresholds. All combinations are presented in @tbl-algos.

For MMSE-based algorithms, the following sets of items served as the basis:

$Global = \{MMSE < 26\}$

$Attention = \{{Sevens\ backwards < 4}\}$

$Executive = \{{Clock\ drawing} < 2, {Lexical\ fluency\ (S)} < 10\}$

$Construction = \{Pentagons < 1\}$

$Memory = \{{Three{\text-}words\ recall} < 3\}$

$IADL = \{FAQ > 7, {FAQ\ (it. 9)} > 1\}$

The ensuing Cartesian product $Global \times Attention \times Executive \times Construction \times Memory \times IADL$ results in $1 \times 1 \times 2 \times 1 \times 1 \times 2 = 4$ MMSE-based algorithms for probable PDD.

For MoCA-based algorithms, the following sets of items served as the basis:

$Global = \{MoCA < 26\}$

$Attention = \{{Sevens\ backwards < 3}\}$

$Executive = \{{Clock\ drawing} < 2, {Clock\ drawing} < 3, {Lexical\ fluency\ (K)} < 11\}$

$Construction = \{Cube\ drawing < 1\}$

$Memory = \{Five{\text-}words\ recall < 1, Five{\text-}words\ recall < 2, Five{\text-}words\ recall < 3, Five{\text-}words\ recall < 4, Five{\text-}words\ recall < 5\}$

$Language = \{Abstraction < 2, Animal\ naming < 3\}$

$IADL = \{FAQ > 7, {FAQ\ (it. 9)} > 1\}$

Note that the additional language domain adds complexity to establishing a diagnostic algorithm because simply by adding it to the set of items, the number of potential algorithms doubles. Further complexity is added by the fact that there are so far no guidelines for selecting a diagnostic threshold for Clock drawing and Five-words recall tests, both of which differ from their counterparts used by @dubois2007. Finally, although the Sevens backwards item has different thresholds in MoCA-based compared to MMSE-based algorithms, this difference is solely due to a difference in scoring whereby 3 points in MoCA correspond to 4 or 5 points in MMSE. The Seven backwards item threshold for MoCA-based algorithms used in this study is thus equivalent to its MMSE-based counterpart.

Computing the Cartesian product $Global \times Attention \times Executive \times Construction \times Memory \times Language \times IADL$ yields $1 \times 1 \times 3 \times 1 \times 5 \times 2 \times 2 = 60$ distinct MoCA-based algorithms for probable PDD.

For sMoCA-based algorithms, the following sets of items served as the basis:

$Global = \{sMoCA < 13\}$

$IADL = \{FAQ > 7, {FAQ\ (it. 9)} > 1\}$

yielding $Global \times IADL$, i.e., $1 \times 2 = 2$ distinct sMoCA-based algorithms for probable PDD.

Finally, the Level II algorithms were based on the following sets of items:

$Attention = \{z(TMT\ A) < -1.5\ \cup z(WAIS\ DSB) < -1.5\}$

$Executive = \{z(CF\ A) < -1.5\ \cup z(PST\ C) < -1.5\}$

$Construction = \{z(JoLO) < -1.5\ \cup z(CLOXI) < -1.5\}$

$Memory = \{z(RAVLT\ DR) < -1.5\ \cup z(BVMTR\ DR) < -1.5\ \cup z(WMS\text-III\ Family\ Pictures) < -1.5\}$

$Language = \{z(WAIS\ Similarities) < -1.5 \ \cup z(BNT\ 60) < -1.5\}$

$IADL = \{FAQ > 7, {FAQ\ (it. 9)} > 1\}$

where $z()$ denotes calculation of age, sex and education adjusted z-score. This yields $1 \times 1 \times 1 \times 1 \times 1 \times 2 = 2$ distinct Level II algorithms for probable PDD in the current study. All but the BNT 60 item were evaluated using regression norms published by @bezdicek2017. Since the original article used BNT 30 instead of BNT 60, we approximated the deficit in BNT 60 by comparing patients' raw score to age- and education-specific normative values reported by @zemanova2016. Specifically, patients whose BNT 60 score fell below 5^th^ percentile of their demographic group in Table 6 of @zemanova2016 were considered to show signs of impaired performance.

### Operationalization of Impaired Cognition

In the original criteria, item 4 of Level I criteria, i.e., impaired cognition, was defined as follows: *"The proposed diagnostic criteria require a profile of cognitive deficits, typical of those described for PD-D, in two or more of four domains."* [@dubois2007, p. 2316] Consequently, we defined impaired cognition as a deficit in two or more domains of four in MMSE-based criteria and as a deficit in two or more of five domains in MoCA-based criteria. sMoCA-based criteria omitted the "impaired cognition" item altogether because they were intended as a shorter screening alternative to classical Level I assessment. Finally, for the Level II criteria, we employed standard definition of impaired cognition as the *"\[i\]mpairment on at least two neuropsychological tests, represented by either two impaired tests in one cognitive domain or one impaired test in two different cognitive domains."* [@litvan2012, Table 1]

```{r}
#| label: tbl-algos
#| tbl-cap: Summary of all algortihms for probable PDD used in the study.

prev$gtables$algorithms
```

## Theoretical and Empirical Estimands

In this study, we follow the framework proposed by @lundberg2021 for specifying targets of inference (i.e., the estimands) in qunatitative sciences to increase transparency and connect statistical evidence to relevant theory. @tbl-estimands contains verbal description of the components relating to each of our proclaimed research objectives and map them to the population quantity of interest (the theoretical estimand), data-dependent quantity that could be estimated (the empirical estimand) and quantities that are reported in the study (statistical estimates).

```{r}
#| label: tbl-estimands
#| tbl-cap: Mapping between research objectives and quantities of interest in the current study.

table_estimands()
```

The RO1 - to estimate the PDD rate and evaluate the diagnostic variability and concordance across different algorithms of probable PDD - was divided into four distinct research objectives:

-   to estimate the rate of PDD within PD (RO1.1),
-   to estimated variability of this rate (RO1.2),
-   to evaluate predictive value of demographic variables for probable PDD classification (RO1.3) and
-   to evaluate concordance between different probable PDD operationalizations and criteria (RO1.4).

Estimates relating to RO1.1 and RO1.3 cannot be safely generalized beyond a population of PD patients that are candidates for DBS due to the systematic differences between DBS candidates pool and general PD population (such as the lower age of DBS candidates compared to the general PD population). On the other hand, the estimates relating to RO1.4 (and to a lesser degree to RO1.2[^16]) may not be substantially influenced by the sample at hand as the primary source of their variance might come from variability in measures employed (e.g., MMSE vs MoCA to assess global cognitive performance) rather than variability in patients' performance. Assuming that there is no substantial Differential Item Functioning for DBS candidates compared to a broader population of patients with PD, the estimates relating to RO1.4 can be cautiously generalized beyond the current sample.

[^16]: Because the quantity of interest is a rate and could thus be though of as a sum of binomially distributed PDD occurences divided by the total number of patients, its variance will likely systematically vary with its mean. Specifically, as the rate goes from extremes to 0.5, the variance increases. Consequently, if our estimate of the rate was lower than the true population rate, e.g., because our sample includes younger patients compared to the general PD population, our estimate of variance would also lower than the true variance of PDD rate in the general PD population. Nonetheless, the between-algorithm variability may not be affected by this phenomenon as unlike variability of PDD rate, we do not have reason to assume it comes about by summing independent binomial events

Finally, for the RO2, the theoretical estimand is defined as the set of diagnostic components whose variation systematically alters the probability of a probable PDD diagnosis. This aspect of the study is exploratory in nature. Empirically, we assess the contribution of each diagnostic feature by examining how variations in operational definitions (e.g., domain-specific thresholds, criteria for functional impairment) influence the statistical estimates derived for the first objective. This allows us to identify the diagnostic elements most responsible for between-algorithm discrepancies.

## Supplementary Presentation of Results

```{r}
#| label: tbl-rates
#| tbl-cap: Estimates of the rate of probable PDD in the sample.

prev$gtables$rates
```

```{r}
#| label: tbl-approx-bal
#| tbl-cap: Level I algorithms for probable PDD as predictors of Level II classification as the reference arranged by their balanced accuracy score.

table_levelII_approximations(conc$table, algos, "Balanced Accuracy")
```

```{r}
#| label: fig-data
#| fig-cap: Representation of study data.
#| apa-note: The figure shows whether patients (x-axis) ordered from the youngest (left) to the oldest (right) were classified as probable PDD by each tested algorithm (y-axis) ordered from the one with the lowest (bottom) to the highest (top) PDD rate estimate. Patients printed in red are women, patients printed in blue are men. Red cells indicate probable PDD diagnosis, grey cells indicate non-PDD diagnosis and white cells indicate missing diagnosis.

pred$plots$data
```

```{r}
#| label: fig-pars
#| fig-cap: Summary of logistic regressions parameters prediction probable PDD by age and sex.
#| apa-note: Histograms represent odd ratio (OR) estimates and p-values associated with age, sex, and their interaction as predictors of each of the 68 probable PDD classification. In the case of parameters for sex, values higher than 20 were omitted for clarity. Vertical lines indicate OR = 1 and p = .05.

pred$plots$parameters
```

```{r}
#| label: fig-kappa
#| fig-cap: Cohen's κ matrix.
#| apa-note: The matrix depicts Cohen's κ measuring agreement between algorithms for PDD. Algorithms printed in red defined IADL deficit by FAQ total score, algorithms printed in black defined IADL deficit by FAQ item 9 response.

conc$plots$Kappa
```

```{r}
#| label: fig-sens
#| fig-cap: Sensitivity matrix.
#| apa-note: The matrix depicts sensitivity of algorithms for PDD depicted on x-axis in predicting outcomes based on algorithms on the y-axis. Algorithms printed in blue defined IADL deficit by FAQ total score, algorithms printed in black defined IADL deficit by FAQ item 9 response.

conc$plots$Sensitivity
```

```{r}
#| label: fig-spec
#| fig-cap: Specificity matrix.
#| apa-note: The matrix depicts specificity of algorithms for PDD depicted on x-axis in predicting outcomes based on algorithms on the y-axis. Algorithms printed in blue defined IADL deficit by FAQ total score, algorithms printed in black defined IADL deficit by FAQ item 9 response.

conc$plots$Specificity
```

